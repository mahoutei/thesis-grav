% =============================================================================
% FILE: chapters/chapter1.tex
% DESCRIPTION: Introduction Chapter
% =============================================================================

\chapter{INTRODUCTION}

The standard cosmological model holds that non-baryonic dark matter constitutes approximately 85 percent of the total matter content of the Universe (Vegetti et al., 2024). Its physical nature remains unknown because dark matter does not interact with light. Gravitational lensing occurs when a massive object, such as a galaxy cluster, warps spacetime, causing light to bend, distort, and magnify as it passes near the massive object (Massey et al., 2010). In particular, strong gravitational lensing by galaxies produces multiple, highly distorted images of background sources. These distortions are sensitive not only to the smooth mass of the lens galaxy but also to small perturbations in the gravitational potential of the lens galaxy. Strong galaxy-galaxy lensing, where a foreground galaxy or cluster magnifies and distorts the image of a background source into arcs or rings, is commonly known as Einstein rings. Consequently, galaxy-scale strong lenses can reveal subgalactic dark matter structures (subhalos) both in the lens and along the line of sight (Diaz Rivero \& Dvorkin, 2020). Gravitational lensing is a distinctive method for testing dark matter theories. For instance, the collisionless cold dark matter (CDM) model forecasts the existence of numerous low-mass subhalos, indicative of a "bottom-up" approach to structure formation. 

The scientific potential of gravitational lensing is inextricably linked to our ability to accurately model lens systems. The core of this challenge lies in solving the lens equation, a complex mapping that relates the true position of a background source to its observed distorted image. Traditionally, this has been accomplished through parametric modeling, where the mass distribution of the lensing galaxy is described by a predefined mathematical profile. The Singular Isothermal Ellipsoid (SIE) has emerged as a particularly effective and widely used model for representing the mass distribution of early type galaxies (Massey et al., 2010). The analytical solutions for the SIE deflection angles, as refined by Keeton (2001), provide a robust framework for fitting the observed lensing features (Keeton, 2001). 
However, this traditional approach is associated with several challenges. This process is often computationally intensive and requires sophisticated numerical optimization techniques to determine the best-fit parameters. It is also susceptible to degeneracies, where different combinations of model parameters can produce nearly identical lensing images, complicating the interpretation of the results. Furthermore, these methods typically require significant expert oversight and are not easily scalable to the massive volumes of data produced by contemporary wide-field astronomical surveys, such as the Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) (Wagner-Carena et al., 2023). The sheer scale of upcoming datasets necessitates a paradigm shift toward automated, efficient, and robust lens analysis methods.

In response to these computational bottlenecks, deep learning has emerged as a transformative force in the field of computational astrophysics. In lens detection, convolutional neural networks (CNNs) have been trained on large sets of simulated images to automatically identify strong lens features in survey data. For instance, Lanusse et al. (2018) introduced DeepLens, a CNN-based lens finder trained on 20,000 realistic LSST-like simulations, achieving a 99 percent non-lens rejection rate while maintaining 90 percent completeness for lenses with Einstein radii larger than 1.4". Similarly, Jacobs et al. (2019) created ensembles of CNNs trained on half a million images from Dark Energy Survey (DES) data, ultimately identifying 84 new high-redshift strong-lens candidates, and demonstrating that CNNs can rapidly sift through large imaging catalogs with minimal human intervention. 
Machine learning has shown even more revolutionary potential in lens modeling. Hezaveh et al. (2017) trained CNNs to perform fast automated analysis of strong lenses, recovering SIE parameters with accuracy comparable to traditional maximum-likelihood models, but with enormous speed gains: on a single GPU they could analyze ~100 lenses per second—roughly $10^7$ times faster than conventional inference. This landmark result showed that neural networks can rapidly invert the lens equation and extract the physical parameters in a single forward pass. Recently, machine learning has begun to tackle substructure detection directly, with Tsang et al. (2024) applying a U-Net to simulated strong-lensing images to flag pixels associated with perturbing subhalos, achieving a ~71\% true-positive rate in identifying lens systems containing subhalos of mass $10^{9}$--$10^{9.5}\,M$.
	
Despite these successes, a critical limitation persists: most of these studies use generic architectures (CNNs, U-Nets) with purely data-driven training. These models do not explicitly enforce known lensing physics, which can limit their robustness and their interpretability. This gap between computational efficiency and physical consistency motivates the exploration of more structured approaches that combine deep learning with astrophysical knowledge.

Two parallel developments in machine learning offer promising solutions to these limitations. First, transformer models, which were originally developed for natural language processing, have recently transformed computer vision. The key innovation is the self-attention mechanism, which relates features at different positions in an image without relying on convolutions. Dosovitskiy et al. (2020) introduced the Vision Transformer (ViT), which splits an image into patches and feeds them into a standard transformer encoder. However, a limitation of vanilla ViT is the quadratic scaling of attention with image size. To address this, Liu et al. (2021) proposed the Swin Transformer, a hierarchical vision transformer with shifted windows that yields linear computational complexity while still capturing multi-scale features. Swin achieved state-of-the-art results in image classification, object detection, and semantic segmentation, suggesting that transformer models may be well-suited for astrophysical image analysis, where long-range dependencies and global lensing geometry are crucial.

Second, physics-informed machine learning has emerged as a powerful paradigm for embedding domain knowledge into the learning algorithms. A canonical example is the Physics-Informed Neural Network (PINN) framework of Raissi et al. (2019), in which a neural network is trained not only to fit data but also to satisfy given partial differential equations. In astrophysics, recent studies have begun to apply these ideas to lensing. LensPINN (Ojha et al., 2024) integrates the gravitational lensing equation directly into a network by combining a vision transformer encoder with convolutional layers. Similarly, Lensformer (Veloso de Souza et al., 2023) introduced a physics-informed vision transformer that embeds known lensing transformations in both the encoder and decoder stages. Both studies reported that physics-informed architectures matched or outperformed larger generic models, emphasizing that encoding the lens equation and known morphologies into the network leveraged domain knowledge to improve learning efficiency.


\section{Thesis Statement and Contribution}

This thesis addresses these limitations by introducing GraviLens, a novel physics-informed deep learning framework for the analysis of gravitational lensing. The core hypothesis is that by integrating a state-of-the-art hierarchical vision architecture with physically grounded models and advanced training stability techniques, we can achieve a model that is more accurate, efficient, robust, and physically consistent than existing approaches.
The primary contributions of this work are threefold:
1.	A Hierarchical Transformer Architecture: We leverage the Swin Transformer architecture (Liu et al., 2021) as the backbone of our model. Its hierarchical design and shifted window attention mechanism are uniquely suited to capture the multi-scale features—from the fine arclets to the global structure—present in gravitational lensing images, overcoming the locality constraints of traditional CNNs while maintaining computational efficiency.
2.	Deep Integration of Physical Laws: We move beyond treating physics as a post-hoc regularization term. Instead, we embed the analytical SIE deflection model directly into the network's forward pass as a differentiable layer. This "physics-informed" approach forces the model to learn a representation of the lensing system that is intrinsically consistent with the laws of general relativity, ensuring that its outputs are not just statistically probable, but physically plausible (Raissi et al., 2019; Metcalf et al., 2022).
3.	A Comprehensive Training Stability Framework: To ensure reliable and reproducible training on complex astronomical data, we develop and integrate a suite of advanced optimization techniques. This includes Exponential Moving Average (EMA) regularization (Tarvainen \& Valpola, 2017), the Lookahead optimizer (Zhang et al., 2019), a cosine annealing scheduler with warmup, and adaptive gradient clipping


\section{Research Aims and Objectives}

To realize the vision of GraviLens, this thesis pursues the following specific aims:
1.	To design and implement the GraviLens architecture, integrating the Swin Transformer backbone with a custom physics-informed encoder.
2.	To implement the SIE deflection physics as a fully differentiable PyTorch module, ensuring seamless integration within the deep learning framework and enabling end-to-end training.
3.	To develop and validate a comprehensive training stability framework, combining EMA, Lookahead, and adaptive clipping to mitigate common training failures in deep models for scientific applications.
4.	To empirically evaluate the performance of the proposed framework on a curated dataset of real galaxy images, benchmarking its accuracy, F1-score, and physical consistency against relevant baseline models.
5.	To demonstrate the model's capability to reconstruct physically plausible lensing images and accurately estimate key lens parameters, thereby validating its utility for automated astronomical analysis.


\section{Scope and Delimitations}

This study is focused on the analysis of strong gravitational lensing systems that can be well-approximated by the Singular Isothermal Ellipsoid (SIE) model. The model is designed and trained on single-band, 64x64 pixel galaxy images from the Real-Galaxy-Tiny-Datasetv2. The scope of this work is to demonstrate the efficacy of the proposed architecture and training framework; it does not extend to more complex lensing phenomena such as multi-plane lensing, strong source substructure, or time-delay cosmography. While the principles are generalizable, the specific implementation is tailored to the defined dataset and problem constraints.

